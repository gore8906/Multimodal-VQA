{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47417d6-01bd-4444-8fb8-2cf9898160bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from transformers import (\n",
    "    InstructBlipForConditionalGeneration,\n",
    "    BlipImageProcessor,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    InstructBlipQFormerConfig,\n",
    "    InstructBlipQFormerModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import autocast\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# --- 기본 설정 ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca73e59-4399-41e1-85a9-4d70fa760223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Pre-training과 동일한 모델 아키텍처 재현 ---\n",
    "# SFT를 시작하기 전, state_dict를 로드할 빈 모델 껍데기를 먼저 만듭니다.\n",
    "# 이 구조는 Pre-training을 저장할 때와 정확히 일치해야 합니다.\n",
    "\n",
    "print(\"--- SFT 모델 아키텍처 재현 시작 ---\")\n",
    "\n",
    "# 1-1) 베이스 모델 로드 (Vision Encoder, Q-Former Config, Tokenizer 등 추출용)\n",
    "print(\"1) 베이스 모델 로드 (Hugging Face Hub)\")\n",
    "base_blip_model = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/instructblip-flan-t5-xl\",\n",
    "    torch_dtype=dtype,\n",
    "    use_safetensors=True\n",
    ")\n",
    "base_llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen-1_8B-Chat\",\n",
    "    torch_dtype=dtype,\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen-1_8B-Chat\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "image_processor = BlipImageProcessor.from_pretrained(\n",
    "    \"Salesforce/instructblip-flan-t5-xl\"\n",
    ")\n",
    "\n",
    "# 1-2) Pre-training때와 동일한 컴포넌트 생성\n",
    "print(\"2) Pre-training과 동일한 모델 컴포넌트 생성 중...\")\n",
    "# Vision Encoder 추출\n",
    "vision_model = base_blip_model.vision_model\n",
    "\n",
    "# Query Tokens 추출\n",
    "query_tokens = base_blip_model.query_tokens\n",
    "\n",
    "# Q-Former 설정을 Pre-training때와 같이 10개 레이어로 축소\n",
    "qformer_config = base_blip_model.qformer.config\n",
    "qformer_config.num_hidden_layers = 10\n",
    "shrunken_qformer = InstructBlipQFormerModel(qformer_config)\n",
    "\n",
    "# LLM 모델은 그대로 사용\n",
    "llm_model = base_llm_model\n",
    "\n",
    "# Projector 생성\n",
    "qformer_hidden_size = shrunken_qformer.config.hidden_size\n",
    "llm_hidden_size = llm_model.config.hidden_size\n",
    "image_proj = nn.Linear(qformer_hidden_size, llm_hidden_size)\n",
    "print(\"모델 로딩 완료\")\n",
    "\n",
    "\n",
    "# 메모리 정리\n",
    "del base_blip_model, base_llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99722a13-b27d-4e5d-807e-64b34cd082c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-3) Pre-training때와 동일한 최종 모델 클래스 정의\n",
    "class VisionLLM_QFormer(nn.Module):\n",
    "    def __init__(self, vision_model, qformer, query_tokens, image_proj, llm_model):\n",
    "        super().__init__()\n",
    "        self.vision_model = vision_model\n",
    "        self.qformer = qformer\n",
    "        self.image_proj = image_proj\n",
    "        self.llm_model = llm_model\n",
    "        self.query_tokens = query_tokens\n",
    "        # 그래디언트 체크포인팅 활성화\n",
    "        self.vision_model.gradient_checkpointing_enable()\n",
    "        self.llm_model.gradient_checkpointing_enable()\n",
    "        self.qformer.gradient_checkpointing_enable()\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask, labels=None):\n",
    "        image_embeds = self.vision_model(pixel_values=pixel_values, return_dict=True).last_hidden_state\n",
    "        image_attention_mask = torch.ones(image_embeds.shape[:-1], dtype=torch.long, device=image_embeds.device)\n",
    "        expanded_query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n",
    "        query_shape = expanded_query_tokens.shape\n",
    "        dummy_query_input_ids = torch.zeros((query_shape[0], query_shape[1]), dtype=torch.long, device=device)\n",
    "        query_outputs = self.qformer(\n",
    "            input_ids=dummy_query_input_ids,\n",
    "            query_embeds=expanded_query_tokens,\n",
    "            encoder_hidden_states=image_embeds,\n",
    "            encoder_attention_mask=image_attention_mask,\n",
    "            return_dict=True\n",
    "        ).last_hidden_state\n",
    "        projected_feats = self.image_proj(query_outputs)\n",
    "        input_embeds = self.llm_model.get_input_embeddings()(input_ids)\n",
    "        full_embeds = torch.cat([projected_feats, input_embeds], dim=1)\n",
    "        full_attention_mask = torch.cat([\n",
    "            torch.ones(projected_feats.shape[:2], dtype=attention_mask.dtype, device=attention_mask.device),\n",
    "            attention_mask\n",
    "        ], dim=1)\n",
    "        full_labels = None\n",
    "        if labels is not None:\n",
    "            image_labels = torch.full((projected_feats.shape[0], projected_feats.shape[1]), -100, dtype=torch.long, device=labels.device)\n",
    "            full_labels = torch.cat([image_labels, labels], dim=1)\n",
    "        return self.llm_model(\n",
    "            inputs_embeds=full_embeds,\n",
    "            attention_mask=full_attention_mask,\n",
    "            labels=full_labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "# --- 3. SFT용 데이터셋 및 Collate 함수 정의 ---\n",
    "class SFTStreamingMultiTurnDataset(IterableDataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, image_processor, max_length=1024):\n",
    "        super().__init__()\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = max_length\n",
    "        self.eos_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    def __iter__(self):\n",
    "        for sample in self.hf_dataset:\n",
    "            try:\n",
    "                if 'image' not in sample or 'conversations' not in sample: continue\n",
    "                \n",
    "                image = sample[\"image\"].convert(\"RGB\")\n",
    "                pixel_values = self.image_processor(image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "                \n",
    "                full_input_ids = []\n",
    "                full_labels = []\n",
    "\n",
    "                for conv in sample[\"conversations\"]:\n",
    "                    role = conv.get(\"from\")\n",
    "                    value = conv.get(\"value\")\n",
    "\n",
    "                    if not value or not isinstance(value, str): continue\n",
    "                    \n",
    "                    # <image> 토큰을 명시적인 지시어로 변경\n",
    "                    if \"<image>\" in value:\n",
    "                        value = value.replace(\"<image>\", \"\").strip()\n",
    "                        # 이미지는 항상 맨 앞에 위치하므로, <image> 토큰에 해당하는 지시어 추가\n",
    "                        full_input_ids.extend(self.tokenizer(\"<image>\\n\").input_ids)\n",
    "                        full_labels.extend([-100] * len(self.tokenizer(\"<image>\\n\").input_ids))\n",
    "\n",
    "                    if role == 'human':\n",
    "                        human_tokens = self.tokenizer(value).input_ids\n",
    "                        full_input_ids.extend(human_tokens)\n",
    "                        full_labels.extend([-100] * len(human_tokens))\n",
    "                    \n",
    "                    elif role == 'gpt':\n",
    "                        gpt_tokens = self.tokenizer(value).input_ids\n",
    "                        gpt_tokens.append(self.eos_token_id)\n",
    "                        full_input_ids.extend(gpt_tokens)\n",
    "                        full_labels.extend(gpt_tokens)\n",
    "\n",
    "                if len(full_input_ids) > self.max_length:\n",
    "                    full_input_ids = full_input_ids[:self.max_length]\n",
    "                    full_labels = full_labels[:self.max_length]\n",
    "\n",
    "                yield {\n",
    "                    \"pixel_values\": pixel_values.squeeze(),\n",
    "                    \"input_ids\": torch.tensor(full_input_ids, dtype=torch.long),\n",
    "                    \"labels\": torch.tensor(full_labels, dtype=torch.long)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"!!! 데이터 처리 중 에러 발생, 샘플 건너뜁니다. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    padded_input_ids = llm_tokenizer.pad({\"input_ids\": input_ids}, padding='longest', return_tensors=\"pt\")\n",
    "    padded_labels = llm_tokenizer.pad({\"input_ids\": labels}, padding='longest', return_tensors=\"pt\").input_ids\n",
    "    padded_labels[padded_labels == llm_tokenizer.pad_token_id] = -100\n",
    "    return {\"pixel_values\": pixel_values, \"input_ids\": padded_input_ids.input_ids, \"attention_mask\": padded_input_ids.attention_mask, \"labels\": padded_labels}\n",
    "\n",
    "# share 전략으로 Pretraining. Vision Encoder Layer 절반 동결, 나머지 전체 학습\n",
    "def freeze_for_pretraining(vision_model, num_freeze_layers=20):\n",
    "    print(\"[학습 전략] Pre-training을 위한 모델 동결을 시작합니다.\")\n",
    "    print(\"   - Vision Encoder의 Embedding 레이어 동결 중...\")\n",
    "    for param in vision_model.embeddings.parameters(): param.requires_grad = False\n",
    "    print(f\"   - Vision Encoder의 앞 {num_freeze_layers}개 Transformer 레이어 동결 중...\")\n",
    "    for layer in vision_model.encoder.layers[:num_freeze_layers]:\n",
    "        for param in layer.parameters(): param.requires_grad = False\n",
    "    print(\"동결/활성화 설정 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce825d-9e99-405b-9c99-705b0623e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-4) 최종 모델 '껍데기' 생성\n",
    "multimodal_model = VisionLLM_QFormer(\n",
    "    vision_model, shrunken_qformer, query_tokens, image_proj, llm_model\n",
    ").to(device, dtype=dtype)\n",
    "\n",
    "print(\"SFT 모델 아키텍처 재현 완료.\")\n",
    "\n",
    "# --- 2. 사전 학습된 가중치 로드 ---\n",
    "PRETRAINED_MODEL_PATH = \"share_instructblip_qwen_pt.pth\"\n",
    "if os.path.exists(PRETRAINED_MODEL_PATH):\n",
    "    print(f\"\\n사전 학습된 모델 가중치를 로드합니다: {PRETRAINED_MODEL_PATH}\")\n",
    "    multimodal_model.load_state_dict(torch.load(PRETRAINED_MODEL_PATH, map_location=device))\n",
    "else:\n",
    "    print(f\"경고: 사전 학습된 모델 파일({PRETRAINED_MODEL_PATH})을 찾을 수 없습니다. 랜덤 가중치로 SFT를 시작합니다.\")\n",
    "\n",
    "\n",
    "# Qwen 토크나이저의 pad_token 설정\n",
    "if llm_tokenizer.pad_token is None:\n",
    "    print(\"\\npad_token이 설정되지 않았습니다. Qwen 모델에 맞게 수동으로 설정합니다.\")\n",
    "    if llm_tokenizer.eos_token is None:\n",
    "        llm_tokenizer.eos_token = '<|endoftext|>'\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "    \n",
    "print(\"--- 토크나이저 설정 확인 ---\")\n",
    "print(f\"pad_token: {llm_tokenizer.pad_token}, pad_token_id: {llm_tokenizer.pad_token_id}\")\n",
    "assert llm_tokenizer.pad_token_id == 151643, \"pad_token_id가 올바르게 설정되지 않았습니다!\"\n",
    "print(\"토크나이저 설정이 올바릅니다.\")\n",
    "\n",
    "\n",
    "# SFT에서는 Vision Encoder 앞 30개 Layer 동결\n",
    "freeze_for_pretraining(\n",
    "    vision_model=multimodal_model.vision_model,\n",
    "    num_freeze_layers=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f654f-40d4-470c-8e23-bb5e1159bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. SFT 학습 실행 ---\n",
    "# 하이퍼파라미터\n",
    "NUM_EPOCHS = 1\n",
    "PER_DEVICE_BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_SAMPLES = 645000\n",
    "SAVE_CHECKPOINT_STEPS = 3\n",
    "\n",
    "print(\"\\nLLaVA-NeXT-Data 멀티턴 데이터셋을 스트리밍합니다...\")\n",
    "streaming_dataset = load_dataset(\"lmms-lab/LLaVA-NeXT-Data\", split=\"train\", streaming=True)\n",
    "train_dataset = SFTStreamingMultiTurnDataset(streaming_dataset, llm_tokenizer, image_processor, max_length=512)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=PER_DEVICE_BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "print(\"\\nSFT에서는 Vision Encoder 앞 30개 Layer만 동결, 나머지 전체 학습\")\n",
    "trainable_params = [p for p in multimodal_model.parameters() if p.requires_grad]\n",
    "total_trainable_params = sum(p.numel() for p in trainable_params)\n",
    "print(f\"총 학습 가능 파라미터 수: {total_trainable_params / 1_000_000:.2f}M\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=LEARNING_RATE, eps=1e-6)\n",
    "\n",
    "loss_history = []\n",
    "print(\"\\nShare_InstructBlip_Qwen SFT 시작\")\n",
    "effective_batch_size = PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "total_steps = NUM_TRAIN_SAMPLES // effective_batch_size\n",
    "multimodal_model.train()\n",
    "progress_bar = tqdm(range(total_steps), desc=f\"Epoch {1} (SFT)\")\n",
    "completed_steps = 0\n",
    "trained_sample_count = 0\n",
    "\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    if completed_steps >= total_steps:\n",
    "        print(\"목표 스텝에 도달하여 학습을 조기 종료합니다.\")\n",
    "        break\n",
    "    \n",
    "    pixel_values = batch[\"pixel_values\"].to(device, dtype=dtype)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "\n",
    "    with autocast(dtype=dtype):\n",
    "        outputs = multimodal_model(pixel_values=pixel_values, input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "        print(f\"경고: 스텝 {step}에서 Loss가 NaN 또는 Inf입니다. 이 배치의 학습을 건너뜁니다.\")\n",
    "        optimizer.zero_grad()\n",
    "        continue\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "        torch.nn.utils.clip_grad_norm_(trainable_params, 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        completed_steps += 1\n",
    "        trained_sample_count += effective_batch_size\n",
    "        current_loss = loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "        loss_history.append(current_loss)\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix({\"loss\": f\"{current_loss:.4f}\"})\n",
    "\n",
    "        if SAVE_CHECKPOINT_STEPS > 0 and (completed_steps % SAVE_CHECKPOINT_STEPS == 0):\n",
    "            checkpoint_path = f\"checkpoint_share_instructblip_qwen_sft_step_{completed_steps}.pth\"\n",
    "            torch.save(multimodal_model.state_dict(), checkpoint_path)\n",
    "            print(f\"SFT Checkpoint 저장 완료: {checkpoint_path}\")\n",
    "            \n",
    "            \n",
    "progress_bar.close()\n",
    "print(\"\\nShare_InstructBlip_Qwen SFT 완료! 최종 모델을 저장합니다...\")\n",
    "torch.save(multimodal_model.state_dict(), \"share_instructblip_qwen_sft.pth\")\n",
    "print(\"최종 SFT 모델 저장 완료! (share_instructblip_qwen_sft.pth)\")\n",
    "\n",
    "# SFT 학습 결과 요약\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SFT 학습 결과 요약\")\n",
    "print(f\"  - 완료된 스텝 수 (Updates): {completed_steps}\")\n",
    "print(f\"  - 실제 학습에 사용된 총 샘플 수: {trained_sample_count:,}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if loss_history:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_history)\n",
    "    plt.title(\"SFT Loss Curve (share_instructblip_qwen_sft)\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"share_instructblip_qwen_sft_loss_curve.png\")\n",
    "    print(\"SFT 학습 곡선 이미지를 'share_instructblip_qwen_sft_loss_curve.png'로 저장했습니다.\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
