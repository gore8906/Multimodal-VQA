{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8eae8-5eb3-4e7c-8042-64dd11f7b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from transformers import (\n",
    "    InstructBlipForConditionalGeneration,\n",
    "    BlipImageProcessor,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    InstructBlipQFormerConfig,\n",
    "    InstructBlipQFormerModel\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.cuda.amp import autocast\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# --- 기본 설정 ---\n",
    "# 사용 가능한 경우 GPU 사용, 그렇지 않으면 CPU 사용\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "# 혼합 정밀도(Mixed Precision) 학습을 위한 데이터 타입 설정\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d9cbbe-7226-4580-9072-2961dbeed696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- 모델 로딩 ---\n",
    "\n",
    "# 1) InstructBLIP 모델 로드 후 Vision Encoder 및 Q-Former 추출\n",
    "print(\"1) InstructBLIP 모델 로드 후 Vision Encoder 및 Q-Former 추출 중... (Salesforce/instructblip-flan-t5-xl)\")\n",
    "full_blip_model = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/instructblip-flan-t5-xl\",\n",
    "    torch_dtype=dtype,\n",
    "    use_safetensors=True\n",
    ").to(device)\n",
    "\n",
    "# Vision Encoder 추출\n",
    "vision_model = full_blip_model.vision_model\n",
    "\n",
    "# query_tokens를 모델 삭제 전에 별도로 추출합니다.\n",
    "print(\"   - Q-Former를 위한 query_tokens 추출 중...\")\n",
    "query_tokens = full_blip_model.query_tokens.to(device, dtype=dtype)\n",
    "\n",
    "# Q-Former 설정을 수정하여 레이어 축소 (12 -> 10)\n",
    "print(\"   - Q-Former 설정을 10개 레이어로 축소 중...\")\n",
    "qformer_config = full_blip_model.qformer.config\n",
    "qformer_config.num_hidden_layers = 10\n",
    "\n",
    "# 축소된 설정으로 새로운 Q-Former 모델 생성\n",
    "shrunken_qformer = InstructBlipQFormerModel(qformer_config).to(device, dtype=dtype)\n",
    "\n",
    "# 사전 학습된 가중치에서 처음 10개 레이어의 가중치를 새로운 Q-Former로 복사\n",
    "print(\"   - 사전 학습된 Q-Former 가중치를 축소된 모델로 로드 중...\")\n",
    "shrunken_qformer.load_state_dict(full_blip_model.qformer.state_dict(), strict=False)\n",
    "\n",
    "# Q-Former를 포함한 전체 BLIP 모델은 더 이상 필요 없으므로 메모리에서 해제\n",
    "del full_blip_model\n",
    "\n",
    "# Image Processor 로드\n",
    "image_processor = BlipImageProcessor.from_pretrained(\n",
    "    \"Salesforce/instructblip-flan-t5-xl\"\n",
    ")\n",
    "\n",
    "# 2) LLM (Qwen) 및 Tokenizer 로드\n",
    "print(\"2) Language Model & Tokenizer 로딩 중... (Qwen/Qwen-1_8B-Chat)\")\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen-1_8B-Chat\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen-1_8B-Chat\",\n",
    "    torch_dtype=dtype,\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True\n",
    ").to(device)\n",
    "print(\"모델 로딩 완료\")\n",
    "\n",
    "\n",
    "# Qwen 토크나이저의 pad_token 설정\n",
    "if llm_tokenizer.pad_token is None:\n",
    "    print(\"pad_token이 설정되지 않았습니다. Qwen 모델에 맞게 수동으로 설정합니다.\")\n",
    "    if llm_tokenizer.eos_token is None:\n",
    "        llm_tokenizer.eos_token = '<|endoftext|>'\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "\n",
    "print(\"--- 토크나이저 설정 확인 ---\")\n",
    "print(f\"pad_token: {llm_tokenizer.pad_token}\")\n",
    "print(f\"pad_token_id: {llm_tokenizer.pad_token_id}\")\n",
    "assert llm_tokenizer.pad_token_id == 151643, \"pad_token_id가 올바르게 설정되지 않았습니다!\"\n",
    "print(\"토크나이저 설정이 올바릅니다.\")\n",
    "\n",
    "\n",
    "# --- 아키텍처 정의 (Q-Former Connector & Multimodal Model) ---\n",
    "print(\"\\n--- 아키텍처 구성 (축소된 Q-Former 사용) ---\")\n",
    "vision_hidden_size = vision_model.config.hidden_size\n",
    "qformer_hidden_size = shrunken_qformer.config.hidden_size\n",
    "llm_hidden_size = llm_model.config.hidden_size\n",
    "\n",
    "print(f\"Vision Encoder Hidden Size: {vision_hidden_size}\")\n",
    "print(f\"Q-Former Hidden Size: {qformer_hidden_size}\")\n",
    "print(f\"LLM Hidden Size: {llm_hidden_size}\")\n",
    "\n",
    "image_proj = nn.Linear(qformer_hidden_size, llm_hidden_size).to(device, dtype=dtype)\n",
    "\n",
    "print(\"Vision Encoder -> Q-Former -> MLP Projector -> LLM 구조 생성 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea33a26-0a37-4e81-a018-ff0fe6374048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 수 계산\n",
    "vision_encoder_params = sum(p.numel() for p in vision_model.parameters())\n",
    "qformer_params = sum(p.numel() for p in shrunken_qformer.parameters())\n",
    "query_token_params = query_tokens.numel() # [수정] 쿼리 토큰 파라미터 추가\n",
    "proj_params = sum(p.numel() for p in image_proj.parameters())\n",
    "llm_params = sum(p.numel() for p in llm_model.parameters())\n",
    "total_params = vision_encoder_params + qformer_params + query_token_params + proj_params + llm_params\n",
    "\n",
    "print(f\"\\n--- 모델 파라미터 수 ---\")\n",
    "print(f\"Vision Encoder: {vision_encoder_params / 1_000_000:.2f}M\")\n",
    "print(f\"Shrunken Q-Former (8-layers): {qformer_params / 1_000_000:.2f}M\")\n",
    "print(f\"Query Tokens  : {query_token_params / 1_000_000:.2f}M\") # [수정] 쿼리 토큰 파라미터 수 출력\n",
    "print(f\"MLP Projector : {proj_params / 1_000_000:.2f}M\")\n",
    "print(f\"Language Model: {llm_params / 1_000_000:.2f}M\")\n",
    "print(f\"-------------------------\")\n",
    "print(f\"전체 파라미터   : {total_params / 1_000_000:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa97c82-b931-4a88-b116-ec311c0ae002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionLLM_QFormer(nn.Module):\n",
    "    def __init__(self, vision_model, qformer, query_tokens, image_proj, llm_model):\n",
    "        super().__init__()\n",
    "        self.vision_model = vision_model\n",
    "        self.qformer = qformer\n",
    "        self.image_proj = image_proj\n",
    "        self.llm_model = llm_model\n",
    "        self.query_tokens = query_tokens\n",
    "\n",
    "        self.vision_model.gradient_checkpointing_enable()\n",
    "        self.llm_model.gradient_checkpointing_enable()\n",
    "        self.qformer.gradient_checkpointing_enable()\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask, labels=None):\n",
    "        image_embeds = self.vision_model(pixel_values=pixel_values, return_dict=True).last_hidden_state\n",
    "        image_attention_mask = torch.ones(image_embeds.shape[:-1], dtype=torch.long, device=image_embeds.device)\n",
    "        expanded_query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n",
    "\n",
    "        # [수정] Q-Former가 요구하는 더미 `input_ids`를 생성합니다.\n",
    "        # 모양만 맞으면 되므로, 0으로 채운 텐서를 사용합니다.\n",
    "        query_shape = expanded_query_tokens.shape\n",
    "        dummy_query_input_ids = torch.zeros((query_shape[0], query_shape[1]), dtype=torch.long, device=device)\n",
    "        \n",
    "        # [수정] Q-Former 호출 시 더미 `input_ids`를 함께 전달합니다.\n",
    "        query_outputs = self.qformer(\n",
    "            input_ids=dummy_query_input_ids,\n",
    "            query_embeds=expanded_query_tokens,\n",
    "            encoder_hidden_states=image_embeds,\n",
    "            encoder_attention_mask=image_attention_mask,\n",
    "            return_dict=True\n",
    "        ).last_hidden_state\n",
    "\n",
    "        projected_feats = self.image_proj(query_outputs)\n",
    "        input_embeds = self.llm_model.get_input_embeddings()(input_ids)\n",
    "        full_embeds = torch.cat([projected_feats, input_embeds], dim=1)\n",
    "\n",
    "        full_attention_mask = torch.cat([\n",
    "            torch.ones(projected_feats.shape[:2], dtype=attention_mask.dtype, device=attention_mask.device),\n",
    "            attention_mask\n",
    "        ], dim=1)\n",
    "\n",
    "        full_labels = None\n",
    "        if labels is not None:\n",
    "            image_labels = torch.full((projected_feats.shape[0], projected_feats.shape[1]), -100, dtype=torch.long, device=labels.device)\n",
    "            full_labels = torch.cat([image_labels, labels], dim=1)\n",
    "\n",
    "        return self.llm_model(\n",
    "            inputs_embeds=full_embeds,\n",
    "            attention_mask=full_attention_mask,\n",
    "            labels=full_labels,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "\n",
    "# --- 데이터셋 및 유틸리티 함수 ---\n",
    "class PreTrainingStreamingDataset(IterableDataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, image_processor, max_length=512):\n",
    "        super().__init__()\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        for sample in self.hf_dataset:\n",
    "            try:\n",
    "                if 'image' not in sample or 'conversations' not in sample: continue\n",
    "                image = sample[\"image\"].convert(\"RGB\")\n",
    "                pixel_values = self.image_processor(image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "                conversations = sample.get(\"conversations\")\n",
    "                if not conversations or not isinstance(conversations, list): continue\n",
    "                    \n",
    "                # instruction = next((c.get(\"value\") for c in conversations if c.get(\"from\") == \"human\"), None)\n",
    "                # response = next((c.get(\"value\") for c in conversations if c.get(\"from\") == \"gpt\"), None)\n",
    "                \n",
    "                # 'human'의 입력은 항상 '<image>'이므로, 이를 기반으로 새로운 instruction을 정의합니다.\n",
    "                human_input = next((c.get(\"value\") for c in conversations if c.get(\"from\") == \"human\"), None)\n",
    "                response = next((c.get(\"value\") for c in conversations if c.get(\"from\") == \"gpt\"), None)\n",
    "                \n",
    "                # human_input이 '<image>'인 경우에만 새로운 instruction을 생성합니다.\n",
    "                if human_input and \"<image>\" in human_input:\n",
    "                    # <image> 토큰을 맨 앞에 두고, 그 뒤에 명확한 지시어를 추가합니다. 줄바꿈(\\n)은 좋은 구분자 역할을 합니다.\n",
    "                    instruction = \"<image>\\nDescribe the image in detail.\" \n",
    "                else:\n",
    "                    # 만약 다른 형태의 human_input이 있다면 원래대로 사용합니다. (안전장치)\n",
    "                    instruction = human_input\n",
    "                \n",
    "                if not isinstance(instruction, str) or not isinstance(response, str) or not instruction or not response: continue\n",
    "                full_text = instruction + response + self.tokenizer.eos_token\n",
    "                full_tokens = self.tokenizer(full_text, truncation=True, max_length=self.max_length, return_tensors=None)\n",
    "                full_input_ids = full_tokens.input_ids\n",
    "                instruction_length = len(self.tokenizer(instruction).input_ids)\n",
    "                labels = list(full_input_ids)\n",
    "                for i in range(min(instruction_length, self.max_length)): labels[i] = -100\n",
    "                yield {\n",
    "                    \"pixel_values\": pixel_values.squeeze(),\n",
    "                    \"input_ids\": torch.tensor(full_input_ids, dtype=torch.long),\n",
    "                    \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"!!! 데이터 처리 중 예외 발생, 해당 샘플을 건너뜁니다. 오류: {e}, 샘플 ID: {sample.get('id', 'N/A')}\")\n",
    "                continue\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    padded_input_ids = llm_tokenizer.pad({\"input_ids\": input_ids}, padding='longest', return_tensors=\"pt\")\n",
    "    padded_labels = llm_tokenizer.pad({\"input_ids\": labels}, padding='longest', return_tensors=\"pt\").input_ids\n",
    "    padded_labels[padded_labels == llm_tokenizer.pad_token_id] = -100\n",
    "    return {\"pixel_values\": pixel_values, \"input_ids\": padded_input_ids.input_ids, \"attention_mask\": padded_input_ids.attention_mask, \"labels\": padded_labels}\n",
    "\n",
    "\n",
    "# share 전략으로 Pretraining. Vision Encoder Layer 절반 동결, 나머지 전체 학습\n",
    "def freeze_for_pretraining(vision_model, num_freeze_layers=20):\n",
    "    print(\"[학습 전략] Pre-training을 위한 모델 동결을 시작합니다.\")\n",
    "    print(\"   - Vision Encoder의 Embedding 레이어 동결 중...\")\n",
    "    for param in vision_model.embeddings.parameters(): param.requires_grad = False\n",
    "    print(f\"   - Vision Encoder의 앞 {num_freeze_layers}개 Transformer 레이어 동결 중...\")\n",
    "    for layer in vision_model.encoder.layers[:num_freeze_layers]:\n",
    "        for param in layer.parameters(): param.requires_grad = False\n",
    "    print(\"동결/활성화 설정 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6ed76-c2e4-4633-a76b-e91b53ebdf09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- 학습 실행 ---\n",
    "NUM_EPOCHS = 1\n",
    "PER_DEVICE_BATCH_SIZE = 16\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_SAMPLES = 557800\n",
    "SAVE_CHECKPOINT_STEPS = 1000\n",
    "\n",
    "print(\"\\nPre-training을 위한 스트리밍 데이터셋을 로드합니다...\")\n",
    "streaming_dataset = load_dataset(\"lmms-lab/LLaVA-ReCap-558K\", split=\"train\", streaming=True)\n",
    "train_dataset = PreTrainingStreamingDataset(streaming_dataset, llm_tokenizer, image_processor, max_length=512)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=PER_DEVICE_BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "multimodal_model = VisionLLM_QFormer(\n",
    "    vision_model, shrunken_qformer, query_tokens, image_proj, llm_model\n",
    ").to(device)\n",
    "\n",
    "freeze_for_pretraining(\n",
    "    vision_model=multimodal_model.vision_model,\n",
    "    num_freeze_layers=20\n",
    ")\n",
    "\n",
    "trainable_params = [p for p in multimodal_model.parameters() if p.requires_grad]\n",
    "total_trainable_params = sum(p.numel() for p in trainable_params)\n",
    "print(f\"총 학습 가능 파라미터 수: {total_trainable_params / 1_000_000:.2f}M\")\n",
    "\n",
    "total_params = [p for p in multimodal_model.parameters()]\n",
    "total__params = sum(p.numel() for p in total_params)\n",
    "print(f\"전체 파라미터 수: {total__params / 1_000_000:.2f}M\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(trainable_params, lr=LEARNING_RATE, eps=1e-6)\n",
    "\n",
    "loss_history = []\n",
    "print(\"\\nShare_InstructBlip_Qwen Pre-training 시작\")\n",
    "effective_batch_size = PER_DEVICE_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "total_steps = NUM_TRAIN_SAMPLES // effective_batch_size\n",
    "multimodal_model.train()\n",
    "progress_bar = tqdm(range(total_steps), desc=f\"Epoch {1}\")\n",
    "completed_steps = 0\n",
    "trained_sample_count = 0\n",
    "\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    if completed_steps >= total_steps:\n",
    "        print(\"목표 스텝에 도달하여 학습을 조기 종료합니다.\")\n",
    "        break\n",
    "    \n",
    "    pixel_values = batch[\"pixel_values\"].to(device, dtype=dtype)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "\n",
    "    with autocast(dtype=dtype):\n",
    "        outputs = multimodal_model(pixel_values=pixel_values, input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "    if torch.isnan(loss):\n",
    "        print(f\"경고: 스텝 {step}에서 Loss가 NaN입니다. 이 배치의 학습을 건너뜁니다.\")\n",
    "        optimizer.zero_grad()\n",
    "        continue\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "        torch.nn.utils.clip_grad_norm_(trainable_params, 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        completed_steps += 1\n",
    "        trained_sample_count += effective_batch_size\n",
    "        current_loss = loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "        loss_history.append(current_loss)\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix({\"loss\": f\"{current_loss:.4f}\"})\n",
    "\n",
    "        if SAVE_CHECKPOINT_STEPS > 0 and (completed_steps % SAVE_CHECKPOINT_STEPS == 0):\n",
    "            checkpoint_path = f\"checkpoint_share_instructblip_qwen_pt{completed_steps}.pth\"\n",
    "            torch.save(multimodal_model.state_dict(), checkpoint_path)\n",
    "            print(f\"Checkpoint 저장 완료: {checkpoint_path}\")\n",
    "            \n",
    "progress_bar.close()\n",
    "print(\"\\nShare_InstructBlip_Qwen Pre-training 완료! 최종 모델을 저장합니다...\")\n",
    "torch.save(multimodal_model.state_dict(), \"share_instructblip_qwen_pt.pth\")\n",
    "print(\"모델 저장 완료!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"학습 결과 요약\")\n",
    "print(f\"  - 목표 학습 샘플 수: {NUM_TRAIN_SAMPLES}\")\n",
    "print(f\"  - 목표 스텝 수 (Updates): {total_steps}\")\n",
    "print(f\"  - 완료된 스텝 수 (Updates): {completed_steps}\")\n",
    "print(f\"  - 실제 학습에 사용된 총 샘플 수: {trained_sample_count:,}\")\n",
    "if trained_sample_count > 0 and total_steps > 0:\n",
    "    usage_percentage = (trained_sample_count / (total_steps * effective_batch_size)) * 100\n",
    "    print(f\"  - 데이터 사용률: {usage_percentage:.2f}%\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if loss_history:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_history)\n",
    "    plt.title(\"Training Loss Curve (share_instructblip_qwen)\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"share_instructblip_qwen_pretraining_loss_curve.png\")\n",
    "    print(\"학습 곡선 이미지를 'share_instructblip_qwen_pretraining_loss_curve'로 저장했습니다.\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
