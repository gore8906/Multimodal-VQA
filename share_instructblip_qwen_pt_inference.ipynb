{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26dd095-e4c9-4e72-9a66-9938369c2ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    InstructBlipForConditionalGeneration,\n",
    "    BlipImageProcessor,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    InstructBlipQFormerConfig,\n",
    "    InstructBlipQFormerModel\n",
    ")\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# --- ê¸°ë³¸ ì„¤ì • ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500876dd-25d0-493c-aa6e-1a053848294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SFT ëª¨ë¸ê³¼ ë™ì¼í•œ ì•„í‚¤í…ì²˜ ì¬í˜„ ---\n",
    "print(\"--- ì¶”ë¡  ëª¨ë¸ ì•„í‚¤í…ì²˜ ë¡œë“œ ---\")\n",
    "\n",
    "# ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (ì»´í¬ë„ŒíŠ¸ ì¶”ì¶œìš©)\n",
    "print(\"1) ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (Hugging Face Hub)\")\n",
    "\n",
    "base_blip_model = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/instructblip-flan-t5-xl\", low_cpu_mem_usage=True\n",
    ")\n",
    "base_llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen-1_8B-Chat\", trust_remote_code=True, low_cpu_mem_usage=True\n",
    ")\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen-1_8B-Chat\", trust_remote_code=True\n",
    ")\n",
    "image_processor = BlipImageProcessor.from_pretrained(\n",
    "    \"Salesforce/instructblip-flan-t5-xl\"\n",
    ")\n",
    "\n",
    "# SFT ëª¨ë¸ê³¼ ë™ì¼í•œ ì»´í¬ë„ŒíŠ¸ ìƒì„±\n",
    "print(\"2) share_instructblip_qwen ì»´í¬ë„ŒíŠ¸ ìƒì„± ì¤‘...\")\n",
    "vision_model = base_blip_model.vision_model\n",
    "query_tokens = base_blip_model.query_tokens\n",
    "qformer_config = base_blip_model.qformer.config\n",
    "qformer_config.num_hidden_layers = 10 # SFT ë•Œì™€ ë™ì¼í•˜ê²Œ 10ê°œ ë ˆì´ì–´\n",
    "shrunken_qformer = InstructBlipQFormerModel(qformer_config)\n",
    "llm_model = base_llm_model\n",
    "image_proj = nn.Linear(shrunken_qformer.config.hidden_size, llm_model.config.hidden_size)\n",
    "print(\"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "del base_blip_model, base_llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7f02fd08-243d-4e04-829f-f8bdde8094f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT ëª¨ë¸ê³¼ ë™ì¼í•œ ìµœì¢… ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜\n",
    "class VisionLLM_QFormer(nn.Module):\n",
    "    def __init__(self, vision_model, qformer, query_tokens, image_proj, llm_model):\n",
    "        super().__init__()\n",
    "        self.vision_model = vision_model\n",
    "        self.qformer = qformer\n",
    "        self.image_proj = image_proj\n",
    "        self.llm_model = llm_model\n",
    "        self.query_tokens = query_tokens\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask, labels=None):\n",
    "        # í•™ìŠµ ì‹œì—ë§Œ ì‚¬ìš©\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d03d8-857c-418b-8654-e4cea55850af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì¢… ëª¨ë¸ ê»ë°ê¸° ìƒì„±\n",
    "inference_model = VisionLLM_QFormer(\n",
    "    vision_model, shrunken_qformer, query_tokens, image_proj, llm_model\n",
    ").to(device)\n",
    "\n",
    "print(\"ì¶”ë¡  ëª¨ë¸ ì•„í‚¤í…ì²˜ ì¬í˜„ ì™„ë£Œ.\")\n",
    "\n",
    "\n",
    "# --- í•™ìŠµëœ SFT ê°€ì¤‘ì¹˜ ë¡œë“œ ---\n",
    "SFT_MODEL_PATH = \"share_instructblip_qwen_sft.pth\"\n",
    "if os.path.exists(SFT_MODEL_PATH):\n",
    "    print(f\"\\ní•™ìŠµëœ SFT ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {SFT_MODEL_PATH}\")\n",
    "    # state_dictë¥¼ ë¡œë“œí•˜ê³ , ëª¨ë¸ì„ dtypeê³¼ deviceì— ë§ê²Œ ì„¤ì •\n",
    "    inference_model.load_state_dict(torch.load(SFT_MODEL_PATH, map_location=device))\n",
    "    inference_model.to(dtype=dtype)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"ì˜¤ë¥˜: SFT ëª¨ë¸ íŒŒì¼({SFT_MODEL_PATH})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¶”ë¡ ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ëª¨ë¸ì„ ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •\n",
    "inference_model.eval()\n",
    "print(\"ëª¨ë¸ì„ ì¶”ë¡  ëª¨ë“œ(eval)ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# Qwen í† í¬ë‚˜ì´ì €ì˜ pad_token ì„¤ì •\n",
    "if llm_tokenizer.pad_token is None:\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "632fd172-93cf-4189-9698-dd8599768f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ì¶”ë¡  í•¨ìˆ˜ ì •ì˜ ===\n",
    "@torch.no_grad()\n",
    "def multimodal_greedy_generate(\n",
    "    model, full_embeds, attention_mask,\n",
    "    max_new_tokens=64, eos_token_id=None\n",
    "):\n",
    "    generated_ids = []\n",
    "    cur_embeds = full_embeds\n",
    "    cur_attention_mask = attention_mask\n",
    "    batch_size = cur_embeds.size(0)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # ëª¨ë¸ì˜ LLM ë¶€ë¶„ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ í† í° ì˜ˆì¸¡\n",
    "        outputs = model.llm_model(\n",
    "            inputs_embeds=cur_embeds,\n",
    "            attention_mask=cur_attention_mask,\n",
    "            use_cache=True, # ì¶”ë¡  ì†ë„ í–¥ìƒì„ ìœ„í•´ use_cache í™œì„±í™”\n",
    "            return_dict=True\n",
    "        )\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        generated_ids.append(next_token_id)\n",
    "        \n",
    "        if eos_token_id is not None and (next_token_id.squeeze(1) == eos_token_id).all():\n",
    "            break\n",
    "\n",
    "        # ë‹¤ìŒ ì…ë ¥ì„ ìœ„í•´ ìƒì„±ëœ í† í°ì˜ ì„ë² ë”©ì„ í˜„ì¬ ì„ë² ë”©ì— ì¶”ê°€\n",
    "        next_embeds = model.llm_model.get_input_embeddings()(next_token_id)\n",
    "        cur_embeds = torch.cat([cur_embeds, next_embeds], dim=1)\n",
    "        # ì–´í…ì…˜ ë§ˆìŠ¤í¬ë„ 1 ì¶”ê°€\n",
    "        cur_attention_mask = torch.cat(\n",
    "            [cur_attention_mask, torch.ones((batch_size, 1), dtype=cur_attention_mask.dtype, device=device)], dim=1\n",
    "        )\n",
    "\n",
    "    return torch.cat(generated_ids, dim=1) if generated_ids else torch.tensor([[]], device=device, dtype=torch.long)\n",
    "\n",
    "prompt_template = \"\"\"You are a helpful visual assistant.  Based on the image, answer the following multiple-choice question by providing only:\n",
    "\n",
    "- The letter (A, B, C, or D) of the correct option on the first line\n",
    "- A brief reason for your choice on the second line\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "Question: What causes the sharp pain often felt in the head when eating something like this too quickly?\n",
    "\n",
    "Options:\n",
    "(A) Freezing of taste buds on the tongue\t\n",
    "(B) Sudden brain pressure due to rapid intake\t\n",
    "(C) Ice touching the tongue causing a shock\t\n",
    "(D) Blood vessel reaction in the palate causing discomfort\n",
    "\n",
    "The letter of the correct answer is: D\n",
    "Reasoning: The model selects this because the sharp pain from eating cold foods is medically known to result from rapid constriction and dilation of blood vessels in the palate.\n",
    "\n",
    "- The letter (A, B, C, or D) of the correct option on the first line\n",
    "- A brief reason for your choice on the second line\n",
    "\n",
    "Now, answer the following question:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Options:\n",
    "(A) {option_A}\n",
    "(B) {option_B}\n",
    "(C) {option_C}\n",
    "(D) {option_D}\n",
    "\n",
    "The letter of the correct answer is: \"\"\"\n",
    "\n",
    "def get_best_option_by_manual_prompting(model, processor, tokenizer, image_path, question, options, prompt_template, device):\n",
    "    try:\n",
    "        raw_image = Image.open(image_path).convert('RGB')\n",
    "        pixel_values = processor(images=raw_image, return_tensors=\"pt\")[\"pixel_values\"].to(device, dtype=dtype)\n",
    "    except Exception as e:\n",
    "        # print(f\"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}, ì—ëŸ¬: {e}\")\n",
    "        return None\n",
    "\n",
    "    full_prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        option_A=options.get('A', ''), option_B=options.get('B', ''),\n",
    "        option_C=options.get('C', ''), option_D=options.get('D', '')\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(full_prompt, return_tensors='pt').to(device)\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    with torch.no_grad(), autocast(dtype=dtype):\n",
    "        # Vision-Language ì„ë² ë”© ìƒì„± (í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•œ ë¡œì§)\n",
    "        image_embeds = model.vision_model(pixel_values=pixel_values, return_dict=True).last_hidden_state\n",
    "        image_attention_mask = torch.ones(image_embeds.shape[:-1], dtype=torch.long, device=device)\n",
    "        \n",
    "        expanded_query_tokens = model.query_tokens.expand(image_embeds.shape[0], -1, -1)\n",
    "        query_shape = expanded_query_tokens.shape\n",
    "        dummy_query_input_ids = torch.zeros((query_shape[0], query_shape[1]), dtype=torch.long, device=device)\n",
    "        \n",
    "        query_outputs = model.qformer(\n",
    "            input_ids=dummy_query_input_ids,\n",
    "            query_embeds=expanded_query_tokens,\n",
    "            encoder_hidden_states=image_embeds,\n",
    "            encoder_attention_mask=image_attention_mask,\n",
    "            return_dict=True\n",
    "        ).last_hidden_state\n",
    "        \n",
    "        projected_feats = model.image_proj(query_outputs)\n",
    "        input_embeds = model.llm_model.get_input_embeddings()(input_ids)\n",
    "        \n",
    "        # ìµœì¢… ì„ë² ë”©ê³¼ ì–´í…ì…˜ ë§ˆìŠ¤í¬ ê²°í•©\n",
    "        full_embeds = torch.cat([projected_feats, input_embeds], dim=1)\n",
    "        full_attention_mask = torch.cat([torch.ones(projected_feats.shape[:2], device=device), attention_mask], dim=1)\n",
    "\n",
    "        # í…ìŠ¤íŠ¸ ìƒì„±\n",
    "        generated_ids = multimodal_greedy_generate(\n",
    "            model=model,\n",
    "            full_embeds=full_embeds,\n",
    "            attention_mask=full_attention_mask,\n",
    "            max_new_tokens=5, # ì •ë‹µ ì˜µì…˜(A,B,C,D)ê³¼ ì•½ê°„ì˜ ì„¤ëª…ì„ í¬í•¨í•  ìˆ˜ ìˆë„ë¡ ê¸¸ì´ ì¡°ì ˆ\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
    "    \n",
    "    # ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ë¨¼ì € ë‚˜ì˜¤ëŠ” ì•ŒíŒŒë²³(A,B,C,D)ì„ ì •ë‹µìœ¼ë¡œ íŒŒì‹±\n",
    "    match = re.search(r'\\b([A-D])\\b', generated_text.upper())\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        # print(f\"ê²½ê³ : ìœ íš¨í•œ ì„ íƒì§€(A,B,C,D)ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ ì¶œë ¥: '{generated_text}'\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765f383c-fe0f-4a88-8b28-096441fc11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ì¶”ë¡  ì‹¤í–‰ ë° ì œì¶œ íŒŒì¼ ìƒì„± ===\n",
    "print(\"\\ntest.csv ë°ì´í„°ë¡œ ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# CSV íŒŒì¼ ë° ì´ë¯¸ì§€ ê²½ë¡œ ì„¤ì • - ìƒëŒ€ê²½ë¡œë¡œ ì§€ì •\n",
    "test_csv_path = \"./open/test.csv\"\n",
    "submission_csv_path = \"./open/sample_submission.csv\"\n",
    "test_img_base_path = \"./open/test_input_images\"\n",
    "\n",
    "try:\n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    submission_df = pd.read_csv(submission_csv_path)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Inferencing for submission\"):\n",
    "        img_filename = row['ID'] + '.jpg'\n",
    "        img_path = os.path.join(test_img_base_path, img_filename)\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            predictions.append('B')\n",
    "            continue\n",
    "\n",
    "        question = row['Question']\n",
    "        options = {'A': row['A'], 'B': row['B'], 'C': row['C'], 'D': row['D']}\n",
    "        \n",
    "        predicted_answer = get_best_option_by_manual_prompting(\n",
    "            inference_model, image_processor, llm_tokenizer,\n",
    "            img_path, question, options, prompt_template, device\n",
    "        )\n",
    "\n",
    "        if predicted_answer is None:\n",
    "            predicted_answer = 'B'\n",
    "        \n",
    "        predictions.append(predicted_answer)\n",
    "\n",
    "    if len(predictions) == len(submission_df):\n",
    "        submission_df['answer'] = predictions\n",
    "        output_submission_path = \"./open/submission.csv\"\n",
    "        submission_df.to_csv(output_submission_path, index=False)\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ ğŸ‰\")\n",
    "        print(f\"  - ì´ {len(predictions)}ê°œì˜ ì˜ˆì¸¡ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "        print(f\"  - ì œì¶œ íŒŒì¼ì´ '{output_submission_path}' ê²½ë¡œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        print(\"=\"*50)\n",
    "    else:\n",
    "        print(f\"\\nì˜¤ë¥˜: ì˜ˆì¸¡ëœ ê²°ê³¼ì˜ ìˆ˜({len(predictions)})ì™€ ì œì¶œ íŒŒì¼ì˜ í–‰ ìˆ˜({len(submission_df)})ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ì œì¶œ íŒŒì¼ ìƒì„± ì˜¤ë¥˜: í•„ìˆ˜ íŒŒì¼({e.filename})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70331df6-d8a7-496f-a269-925a33c3146c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === train.csvë¥¼ ì´ìš©í•œ ì •í™•ë„ í‰ê°€ ===\n",
    "print(\"\\ntrain.csv ë°ì´í„°ë¡œ ì •í™•ë„ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# CSV íŒŒì¼ ë° ì´ë¯¸ì§€ ê²½ë¡œ ì„¤ì • - ìƒëŒ€ê²½ë¡œë¡œ ì§€ì •ë¨\n",
    "train_csv_path = \"./open/train.csv\"\n",
    "train_img_base_path = \"./open/train_input_images\"\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"í‰ê°€ ì˜¤ë¥˜: train.csv íŒŒì¼ì„ '{train_csv_path}' ê²½ë¡œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    train_df = None\n",
    "\n",
    "if train_df is not None:\n",
    "    correct_predictions = 0\n",
    "    total_valid_samples = 0\n",
    "\n",
    "    for index, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Evaluating\"):\n",
    "        img_filename = row['ID'] + '.jpg'\n",
    "        img_path = os.path.join(train_img_base_path, img_filename)\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            continue # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°\n",
    "\n",
    "        question = row['Question']\n",
    "        options = {'A': row['A'], 'B': row['B'], 'C': row['C'], 'D': row['D']}\n",
    "        true_answer = row['answer']\n",
    "        \n",
    "        # ëª¨ë¸ì„ í†µí•´ ê°€ì¥ ì¢‹ì€ ì„ íƒì§€ ì˜ˆì¸¡\n",
    "        predicted_answer = get_best_option_by_manual_prompting(\n",
    "            inference_model,\n",
    "            image_processor,\n",
    "            llm_tokenizer,\n",
    "            img_path,\n",
    "            question,\n",
    "            options,\n",
    "            prompt_template,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        print(predicted_answer)\n",
    "\n",
    "        if predicted_answer is None:\n",
    "            continue # ëª¨ë¸ì´ ìœ íš¨í•œ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í•˜ë©´ ê±´ë„ˆë›°ê¸°\n",
    "        \n",
    "        total_valid_samples += 1\n",
    "        if predicted_answer == true_answer:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    # ìµœì¢… í‰ê°€ ê²°ê³¼ ì¶œë ¥\n",
    "    accuracy = (correct_predictions / total_valid_samples) * 100 if total_valid_samples > 0 else 0\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"í‰ê°€ ì™„ë£Œ\")\n",
    "    print(f\"  - ì „ì²´ ìœ íš¨ ìƒ˜í”Œ ìˆ˜: {total_valid_samples}\")\n",
    "    print(f\"  - ì •ë‹µ ê°œìˆ˜: {correct_predictions}\")\n",
    "    print(f\"  - ì •í™•ë„ (Accuracy): {accuracy:.2f}%\")\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
